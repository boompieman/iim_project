<html>
<body>
<b>PACKAGE UNDER DEVELOPMENT</b>
<p>
Various viewers and calculation packages for inspecting a corpus. Can be used for reliability and quality analysis, or just to get a grip on what's in a corpus. This overview document is the main documentation of these packages. If you want to use this stuff, please read this text carefully, referring to the documentation of individual classes when necessary.

<h2>Package overview</h2>
<h3>data package</h3>
Contains interfaces to describe the data in your annotations in a way that can be handled by the other packages. 
Basically, an annotation by an human annotator is supposed to be a classification that assigns Values to Items. The precise interpretation of Item and Value is dependent on the actual data.
<h3>calc package</h3>
Contains the interfaces and classes necessary for calculating: kappa, alpha, distance metric interface, confusion matrices, etc.
<h3>impl package</h3>
In this package, a number of different implementations for Items and Values and some corresponding metrics are defined. StringValue, SetValue, DiceMetric, etc.
<h3>view package</h3>
This package contains some GUI elements that can be used to (interactively) view results from the data inspection such as confusion matrices and coincidence matrices.
<h3>timespan package</h3>
Classes and programs to investigate annotations that are non-overlapping, possibly (but not necessarily) continuous, and potentially multi-label. Calculates different types of confusion matrices, kappa and alpha, and visualises the annotations (in a very naive way).

<h2>Summary of the core datainspection packages</h2>
Reliability analysis often consists of 
<ol>
<li>finding out whether separate annotators identified the same Items (segments, units for labeling), 
<li>finding out whether comparable Items have been assigned the same Values (labels, classes, categories) by the annotators and 
<li>finding out where disagreement lies, i.e. what Values are confused with each other in what situations; what type of Items are most often NOT identified by both annotators at the same time; etc.
<li>(Investigating the nature of the errors that annotators made, and deciding how important these errors are, given the use for which the annotations were created.)
</ol>
<p>
The package <code>nite.datainspection.data</code> contains the basic interfaces to describe the annotations in your corpus as {@link net.sourceforge.nite.datainspection.data.Classification Classifications} that assign {@link net.sourceforge.nite.datainspection.data.Value Values} to {@link net.sourceforge.nite.datainspection.data.Item Items}. 

[[INSERT PICTURE HERE WITH EXAMPLE OF WHAT AN CLASSIFICATION IS, ESP THE RELATION BETWEEN THE ITEMS OF CLASSIFICATIONS OF TWO DIFFERENT CODERS]]
[[ALSO A PICTURE THAT SHOWS THAT INDEPENDENT CODINGS CLEARLY LEAD TO SEPARATE CLASSIFICATIONS]]
[[NOTE THAT THIS 'CLASSIFICATION' IS ONLY ONE OF MANY WAYS TO LOOK AT ANNOTATIONS...]

<p>
The package <code>nite.datainspection.calc</code> allows one to create a {@link net.sourceforge.nite.datainspection.calc.ConfusionMatrix ConfusionMatrix} or {@link net.sourceforge.nite.datainspection.calc.CoincidenceMatrix CoincidenceMatrix} from two classifications, or a {@link net.sourceforge.nite.datainspection.calc.CoincidenceMatrixM Multi-annotator CoincidenceMatrix} for more classifications. Such matrices are a source of information about (dis)agreements between annotators as well as a first step towards calculating reliability measures such as kappa or alpha (available through methods in the ConfusionMatrix and CoincidenceMatrix classes). For calculating certain variations of Alpha reliability, a {@link net.sourceforge.nite.datainspection.calc.DistanceMetric DistanceMetric} is needed.

[[INSERT PICTURE HERE THAT EXPLAINS WHAT IS THE RELATION BETWEEN THE CLASSIFICATION AND THE CONFUSION MATRIX]]

<p>
For a specific corpus, one needs to define what constitutes an Item and a Value. For example, when analysing dialogue act annotations, Items may be segments in the transcription, and Values may be Strings denoting a dialogue act label assigned to a transcription segment. As another example, when analysing a segmentation and labeling of the timeline with hand gestures, Items may be segments, or Items may be discretized timespans of e.g. 1 second, whereas a Value would be an assigned gesture label. When an annotation allows multiple labels to be assigned to a segment, Values may be sets of Strings, denoting the set of labels assigned to the segment. Furthermore one needs to define the appropriate DistanceMetrics, something which can be very corpus-specific. The package <code>nite.datainspection.calc.impl</code> offers a number of implementations of the interfaces Value, Item and DistanceMetric which may be sufficient for your corpus. If they are not adequate, you can make your own implementations of those interfaces.
<h2>How to use the datainspection packages</h2>
Reliability:
<ul>
<li>Figure out what are Items, Values and DistanceMetric; if necessary create own implementations of the interfaces
<li>Derive Classifications from data in NXT format
<li>Create coincidencematrix and calculate reliability values and print confusion matrices and start analysing what went wrong and why.
</ul>
However, besides calculating some reliability value, one needs also to investigate the corpus in a more anecdotical way. Finding places where disagreement occurs, looking at what kind of situatations it happens, building informal hypotheses about sources of disagreement, etc. For this you might want to use some of the generic datainspection tools described below.
<h2>The generic datainspection tools</h2>
The datainspection packages contain a few tools which may help in investigating the data in a corpus. Each of these tools is documented in its respective subpackage documentation. Below, a list of (very short) summaries is provided for each tool. 

[[EXPLAIN ABOUT SINGLE CODER/MULTI CODER INSPECTION, mail from myrosia]]

<h3>Timespan tool</h3>
See timespan package. Classes and programs to investigate annotations that are non-overlapping, possibly (but not necessarily) continuous, and potentially multi-label.
<!--
<h3>discourse and relational tool (Rieks)</h3>
<h3>A variation on the corpus scripts of Thomas?</h3>
<h3>More?</h3>
<h3>single coder multiple observation inspection</h3>
<h3>Another type of inspection would be to open corpus in AMIGRAM...</h3>
<h3>Viz of metadata strcuture?</h3>
-->
</body>
</html>